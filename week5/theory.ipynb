{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Exercises\n",
    "Remember to read them all before class and try to solve them and figure out where you may have problems.\n",
    "Exercise 4 is the most important. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: 1 Install and test Pytorch\n",
    "Later in the course we will be using the deep learning frame work pytorch. So install it. Install torchviz too to help plot computation graphs.\n",
    "\n",
    "Let us use automatic differentation for another gradient descent algorithm.\n",
    "\n",
    "Lets do a similar exercise to last time just with some data and Linear Regression Gradient Descent.\n",
    "First we need to understand that to represent data we must use torch tensors. Tensors are very much like numpy arrays just with some extra functionality.\n",
    "The thing we will consider is the backward function that computes gradients of whatever computation you have made using torch tensors.\n",
    "\n",
    "To see how this works, lets see an example.\n",
    "Lets evaluate the gradient of the sigmoid function without actually knowing the formula. All you need to know is how to compute the function using standard functions i.e. $s(z) = 1/(1+e^{-z})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of f(z)=1/(1 + e^{-z}) at z = 0 tensor([0.2500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "z = torch.zeros(1, requires_grad=True)\n",
    "sz = 1.0 / (1+ torch.exp(-z))\n",
    "sz.backward() # compute gradient of sz relative to z in this case\n",
    "print('Gradient of f(z)=1/(1 + e^{-z}) at z = 0', z.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: Gradient Descent with pytorch (manually)\n",
    "In deep learning frameworks all we usually need to do is to show how to compute the cost in a given point and the system then automatically computes the gradient in that data point for you. We will se how later in this course, in this exercise we will try and see if we can use this functionality to do gradient descent.\n",
    "The example will be similar to last weeks gradient descent except now we actually make a data set to run gradient descent on for Linear Regression.\n",
    "\n",
    "\n",
    "**Setup:**\n",
    "We create a data set $D$ that will consist of $n=100$ data points in 2D $(x_1,x_2)$ i.e. two features $x_1, x_2$. \n",
    "The data feature vectors of $x_1$ and $x_2$ are made orthogonal and $x_1$ has unit norm while $x_2$ has norm $a$.\n",
    "\n",
    "We generate a target vector \n",
    "$$\n",
    "y = x_1+x_2\n",
    "$$ \n",
    "which is also a vector of length $n$ i.e. the data we are trying to fit comes from *(a very simple)* linear model.\n",
    "\n",
    "Remember linear regression the in sample error/cost is \n",
    "$$\n",
    "\\textrm{E}_\\textrm{in}(w) = \\frac{1}{n} \\sum_{i=1}^n (w^\\intercal x_i - y_i) = \\frac{1}{n} \\|Xw -y|^2\n",
    "$$\n",
    "\n",
    "\n",
    "We have written the code to generate data and the surrounding Gradient Descent for loop, all you need is to write the code for computing the cost (ein).\n",
    "You can only use commands from torch here (no numpy), but you can use standard operators like $+,-,*,/,**$ on torch tensors that work like their numpy equivalent and torch.sum may be very handy\n",
    "**Complete the gradient descent code below by computing cost using standard operations and torch commands only**\n",
    "\n",
    "The gradient descent will start the search at $w=(42, 2)$ for some reason. We have also sat an almost arbitrary learning rate. You can change both if you like.\n",
    "\n",
    "\n",
    "To see how this linear regression exercise relates to the gradient descent exercie from last week try increasing the value of $a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Cost 17.3\n",
      "epoch 1: Cost 15.190625\n",
      "epoch 2: Cost 13.692634062500002\n",
      "epoch 3: Cost 12.35692604140625\n",
      "epoch 4: Cost 11.152098704369143\n",
      "epoch 5: Cost 10.064767998773146\n",
      "epoch 6: Cost 9.083453075615967\n",
      "epoch 7: Cost 8.197816399012341\n",
      "epoch 8: Cost 7.398529300039392\n",
      "epoch 9: Cost 6.67717269328278\n",
      "epoch 10: Cost 6.0261483556875985\n",
      "epoch 11: Cost 5.438598891008053\n",
      "epoch 12: Cost 4.90833549913477\n",
      "epoch 13: Cost 4.429772787969129\n",
      "epoch 14: Cost 3.997869941142138\n",
      "epoch 15: Cost 3.608077621880779\n",
      "epoch 16: Cost 3.2562900537474038\n",
      "epoch 17: Cost 2.938801773507031\n",
      "epoch 18: Cost 2.652268600590096\n",
      "epoch 19: Cost 2.3936724120325605\n",
      "epoch 20: Cost 2.1602893518593858\n",
      "epoch 21: Cost 1.9496611400530959\n",
      "epoch 22: Cost 1.7595691788979186\n",
      "epoch 23: Cost 1.588011183955372\n",
      "epoch 24: Cost 1.433180093519723\n",
      "epoch 25: Cost 1.2934450344015502\n",
      "epoch 26: Cost 1.1673341435473992\n",
      "epoch 27: Cost 1.0535190645515278\n",
      "epoch 28: Cost 0.9508009557577541\n",
      "epoch 29: Cost 0.858097862571373\n",
      "epoch 30: Cost 0.7744333209706642\n",
      "epoch 31: Cost 0.6989260721760243\n",
      "epoch 32: Cost 0.6307807801388621\n",
      "epoch 33: Cost 0.569279654075323\n",
      "epoch 34: Cost 0.5137748878029789\n",
      "epoch 35: Cost 0.4636818362421884\n",
      "epoch 36: Cost 0.41847285720857486\n",
      "epoch 37: Cost 0.3776717536307389\n",
      "epoch 38: Cost 0.34084875765174183\n",
      "epoch 39: Cost 0.30761600378069703\n",
      "epoch 40: Cost 0.27762344341207906\n",
      "epoch 41: Cost 0.2505551576794013\n",
      "best w found tensor([5.7553, 0.2500], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch # the torch tensor library\n",
    "\n",
    "# CREATE SOME DATA\n",
    "n = 100\n",
    "x1 = np.random.rand(n)\n",
    "x2 = np.random.rand(n)\n",
    "# Grahm schmidt process\n",
    "x1 = x1/np.linalg.norm(x1)\n",
    "x2 = x2/np.linalg.norm(x2)\n",
    "x2 = x2 - np.dot(x1, x2) * x1 #\n",
    "x2 = x2/np.linalg.norm(x2)\n",
    "\n",
    "# CREATE THE DATA MATRIX\n",
    "a = 4.0\n",
    "D = np.c_[x1, a*x2]\n",
    "# CREATE TARGET FUNCTION VECTOR\n",
    "y = x1 + x2\n",
    "\n",
    "# MAKE TORCH VARIABLES TO USE\n",
    "X = torch.from_numpy(D).double()\n",
    "ty = torch.from_numpy(y).double()\n",
    "ni = torch.tensor(1./n, dtype=torch.double)\n",
    "\n",
    "def torch_gd():\n",
    "    w = torch.tensor([42.0, 2.0], dtype=torch.double)\n",
    "    lr = torch.tensor(10.0/a, dtype=torch.double)\n",
    "    epochs = 42\n",
    "    cost_hist = []\n",
    "    for i in range(epochs):\n",
    "        w.requires_grad_() # say i want gradient relative to w in upcoming computation\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE - Compute Ein for linear regression as function of w and store in cost 1 - 4 lines\n",
    "        cost = (((X @ w) - ty) **2).mean()\n",
    "        ### END CODE\n",
    "        cost_hist.append(cost)\n",
    "        print('epoch {0}: Cost {1}'.format(i, cost))\n",
    "        cost.backward() # compute gradient cost as function of w\n",
    "        w = w - lr * w.grad # update w\n",
    "        w.detach_() # removes w from current computation graph\n",
    "    print('best w found', w)\n",
    "torch_gd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3:  Bias Variance \n",
    "\n",
    "-   Does Bias and Variance terms (two numbers) in the Bias Variance\n",
    "    decomposition depend on the learning algorithm.\n",
    "    <br>\n",
    "    Eout[g] = Expected[(g(x) - (g'(x))^2] + Expected[(g'(x) - f(X))^2] <br>\n",
    "                            Var                         Bias\n",
    "    Yes.\n",
    "\n",
    "-   What is Variance (in Bias Variance tradoff) if we have a hypothesis\n",
    "    set of size $1$ namely the constant model $h(x) = 2$. The learning\n",
    "    algorithm always picks this hypothesis no matter the data.\n",
    "    <br>\n",
    "\n",
    "-   What is the Variance (in the Bias Variance tradeoff) if the simple\n",
    "    hypothesis from the previous question is replaced by a very very\n",
    "    sophisticated hypothesis.\n",
    "    <br>\n",
    "    It's the same.\n",
    "\n",
    "-   Assume the target function is a second degree polynomial, and the\n",
    "    input to your algorithm is always eleven (noiseless) points. Your\n",
    "    hypothesis set is the set of all degreee 10 polynomial and the\n",
    "    learning algorithm returns the hypothesis with the best fit\n",
    "    (miniming least squared error) given the data. What is Bias and what\n",
    "    is Variance?\n",
    "    <br>\n",
    "    Can only fit one 10th order polynomial (assuming 10 points are distinct). 2nd order polynomial subset of 10th order polynomial, can set all the other degrees to 0.\n",
    "    Bias --> 0, Variance --> 0, because g'(x) = f\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Bias Variance Experiment \n",
    "In this exercise you must redo the experiment shown at the lectures.\n",
    "This exercise takes up quite a lot of space so we have moved it to a separate notebook. Go to [BiasVariance Notebook](BiasVariance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Bias Variance - Hard Exercise\n",
    "Book Problem 2.24 part (a)\n",
    "\n",
    "Short Version:\n",
    "   \n",
    "  - The target function is $f(x) = x^2$ and the cost is Least Squares.\n",
    "\n",
    "  - Sample two points $x_1, x_2$ from $[-1, 1]$ uniformly at random to get the data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$\n",
    "\n",
    "  - Use hypothesis space $\\{h(x) = ax +b\\mid a,b\\in\\Bbb R\\}$ i.e. lines. There are two parameters $a$ and $b$.\n",
    "\n",
    "  - Given a data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$ the algorithm returns the line that fits these points.\n",
    "\n",
    "  - Your task is to write down an analytical expression for $\\bar{g} = \\mathbb{E}_D [h_D]$ where $h_D$ is the hypothesis learned on D.\n",
    "\n",
    "**Step 1.** What is the in sample error of $h_D$ and why?\n",
    "\n",
    "**Step 2.** Given $D$ what are $a, b$ (defined by the line between $(x_1, x_1^2)$ and  $(x_2, x_2^2)$)? Hint: $x_2^2- x_1^2 = (x_2-x_1)(x_2 + x_1)$.\n",
    "\n",
    "**Step 3.** What is the expected value of the slope $a$ over $x_1$ and $x_2$?\n",
    "\n",
    "**Step 4.** What is the expected value of the intercerpt $b$ over $x_1$ and $x_2$? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**More hints**\n",
    "For the uniform distribution over $[-1,1]$ the mean is $0$ and the variance is $\\frac{1}{2}\\int_{-1}^1 (x-0)^2 dx = \\frac{1}{3}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6: Regularization with Weight decay\n",
    "If we use weight decay regularization ($\\lambda||w||^2)$  for some real number $\\lambda$ in Linear Regression what \n",
    "happens to the optimal weight vector if we let $\\lambda \\rightarrow \\infty$? (cost is $\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2$)\n",
    "\n",
    "||w|| = 0, then 0*inf = 0\n",
    "\n",
    "What if we set $\\lambda = -7?$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 7: Grid Search For Regularization and Validation - Sklearn\n",
    "In this exercise you must we will optimize a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using regularization and validation.\n",
    "You must use the in grid search module [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) from sklearn.\n",
    "\n",
    "In the cell below we have shown an example of how to use the gridsearch module to find a good height decision tree for Decision Trees.\n",
    "\n",
    "Your job is to find a good hyperparameters for decision trees for the industri codes data set!\n",
    "\n",
    "### Task 1:\n",
    "For the industri code text classification data set (See week 2 for more info) find the best combination of max_depth and  min_samples_split  (cell two below)\n",
    "\n",
    "The **max_depth** parameter controls the max depth of a tree and the deeper the tree the more complex the model.\n",
    "\n",
    "The **min_samples_split** controls how many elements the algorithm that constructs the tree is allowed to try and split.\n",
    "So if a subtree contains less than min_leaf_size elements it many not be split into a larger subtree by the algorithm.\n",
    "\n",
    "\n",
    "**Hint:** For the wine data set it is enough to test numbers between 1 and 20 for max_depth, results may differ due to randomness in data split.\n",
    "\n",
    "### Task 2:\n",
    "- How important does the max_depth value seem to be for Decision Trees for Wine classification?\n",
    "- How important is max_depth and min_leaf_size for industri codes classification with Trees?\n",
    "- How long time does it take to use grid search validation for $k$ hyperparamers where we test each parameter for $d$ values?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>2.697398e-06</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3}</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.997222</td>\n",
       "      <td>0.003928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>5.150430e-07</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 9}</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.887640</td>\n",
       "      <td>0.040733</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>1.171463e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870787</td>\n",
       "      <td>0.042441</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>1.798266e-06</td>\n",
       "      <td>11</td>\n",
       "      <td>{'max_depth': 11}</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.870787</td>\n",
       "      <td>0.042441</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>8.778064e-07</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 7}</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.865169</td>\n",
       "      <td>0.034490</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>3.184520e-04</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.062196</td>\n",
       "      <td>6</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.679708</td>\n",
       "      <td>0.009136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1       0.000451      0.000028         0.000162    2.697398e-06   \n",
       "4       0.000419      0.000043         0.000147    5.150430e-07   \n",
       "2       0.000431      0.000037         0.000155    1.171463e-05   \n",
       "5       0.000427      0.000040         0.000148    1.798266e-06   \n",
       "3       0.000422      0.000046         0.000147    8.778064e-07   \n",
       "0       0.001224      0.001241         0.000390    3.184520e-04   \n",
       "\n",
       "  param_max_depth             params  split0_test_score  split1_test_score  \\\n",
       "1               3   {'max_depth': 3}           0.950000           0.833333   \n",
       "4               9   {'max_depth': 9}           0.900000           0.833333   \n",
       "2               5   {'max_depth': 5}           0.850000           0.833333   \n",
       "5              11  {'max_depth': 11}           0.850000           0.833333   \n",
       "3               7   {'max_depth': 7}           0.850000           0.833333   \n",
       "0               1   {'max_depth': 1}           0.566667           0.583333   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "1           0.931034         0.904494        0.051327                1   \n",
       "4           0.931034         0.887640        0.040733                2   \n",
       "2           0.931034         0.870787        0.042441                3   \n",
       "5           0.931034         0.870787        0.042441                3   \n",
       "3           0.913793         0.865169        0.034490                5   \n",
       "0           0.706897         0.617978        0.062196                6   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "1            1.000000            1.000000            0.991667   \n",
       "4            1.000000            1.000000            1.000000   \n",
       "2            1.000000            1.000000            1.000000   \n",
       "5            1.000000            1.000000            1.000000   \n",
       "3            1.000000            1.000000            1.000000   \n",
       "0            0.677966            0.669492            0.691667   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "1          0.997222         0.003928  \n",
       "4          1.000000         0.000000  \n",
       "2          1.000000         0.000000  \n",
       "5          1.000000         0.000000  \n",
       "3          1.000000         0.000000  \n",
       "0          0.679708         0.009136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found {'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image, display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def show_result(clf):\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df = df.sort_values('mean_test_score', ascending=False)\n",
    "    display(df)\n",
    "    print('best parameter found', clf.best_params_)\n",
    "    best_tree = DecisionTreeClassifier(**clf.best_params_)\n",
    "    best_tree.fit(X, y)\n",
    "    return best_tree\n",
    "    \n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "# grid search validation\n",
    "reg_parameters = {'max_depth': list(range(1, 12, 2))}  # dict with all parameters we need to test\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "clf.fit(X, y)\n",
    "# code for showing the result\n",
    "bt = show_result(clf)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.030818</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>19</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 256}</td>\n",
       "      <td>0.940687</td>\n",
       "      <td>0.939579</td>\n",
       "      <td>0.930078</td>\n",
       "      <td>0.936784</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933167</td>\n",
       "      <td>0.934554</td>\n",
       "      <td>0.948448</td>\n",
       "      <td>0.938723</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.030393</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>19</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 64}</td>\n",
       "      <td>0.941796</td>\n",
       "      <td>0.937916</td>\n",
       "      <td>0.930078</td>\n",
       "      <td>0.936599</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>2</td>\n",
       "      <td>0.934276</td>\n",
       "      <td>0.935108</td>\n",
       "      <td>0.949002</td>\n",
       "      <td>0.939462</td>\n",
       "      <td>0.006754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.030607</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>19</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 128}</td>\n",
       "      <td>0.940687</td>\n",
       "      <td>0.937916</td>\n",
       "      <td>0.929523</td>\n",
       "      <td>0.936044</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>3</td>\n",
       "      <td>0.933167</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>0.948725</td>\n",
       "      <td>0.938908</td>\n",
       "      <td>0.006975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.032033</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 16}</td>\n",
       "      <td>0.941242</td>\n",
       "      <td>0.937916</td>\n",
       "      <td>0.928968</td>\n",
       "      <td>0.936044</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>3</td>\n",
       "      <td>0.934554</td>\n",
       "      <td>0.935385</td>\n",
       "      <td>0.949279</td>\n",
       "      <td>0.939739</td>\n",
       "      <td>0.006754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.030172</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 32}</td>\n",
       "      <td>0.940687</td>\n",
       "      <td>0.937916</td>\n",
       "      <td>0.928968</td>\n",
       "      <td>0.935860</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.934276</td>\n",
       "      <td>0.935108</td>\n",
       "      <td>0.949002</td>\n",
       "      <td>0.939462</td>\n",
       "      <td>0.006754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.028520</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>17</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 256}</td>\n",
       "      <td>0.941796</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.925083</td>\n",
       "      <td>0.934381</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>6</td>\n",
       "      <td>0.933167</td>\n",
       "      <td>0.932335</td>\n",
       "      <td>0.944013</td>\n",
       "      <td>0.936505</td>\n",
       "      <td>0.005320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.029229</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>19</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 512}</td>\n",
       "      <td>0.941796</td>\n",
       "      <td>0.937916</td>\n",
       "      <td>0.922863</td>\n",
       "      <td>0.934196</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>7</td>\n",
       "      <td>0.932335</td>\n",
       "      <td>0.934276</td>\n",
       "      <td>0.941242</td>\n",
       "      <td>0.935951</td>\n",
       "      <td>0.003824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.028502</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>17</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 128}</td>\n",
       "      <td>0.941242</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.925083</td>\n",
       "      <td>0.934196</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>7</td>\n",
       "      <td>0.933167</td>\n",
       "      <td>0.932612</td>\n",
       "      <td>0.944290</td>\n",
       "      <td>0.936690</td>\n",
       "      <td>0.005379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.028950</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>17</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 64}</td>\n",
       "      <td>0.940687</td>\n",
       "      <td>0.935144</td>\n",
       "      <td>0.925083</td>\n",
       "      <td>0.933641</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>9</td>\n",
       "      <td>0.934276</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.944568</td>\n",
       "      <td>0.937244</td>\n",
       "      <td>0.005209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 32}</td>\n",
       "      <td>0.939579</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.925083</td>\n",
       "      <td>0.933641</td>\n",
       "      <td>0.006199</td>\n",
       "      <td>9</td>\n",
       "      <td>0.934276</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.944568</td>\n",
       "      <td>0.937244</td>\n",
       "      <td>0.005209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.031870</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 16}</td>\n",
       "      <td>0.939579</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.925083</td>\n",
       "      <td>0.933641</td>\n",
       "      <td>0.006199</td>\n",
       "      <td>9</td>\n",
       "      <td>0.934554</td>\n",
       "      <td>0.933167</td>\n",
       "      <td>0.944845</td>\n",
       "      <td>0.937522</td>\n",
       "      <td>0.005209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.029390</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>17</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 512}</td>\n",
       "      <td>0.941796</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.921199</td>\n",
       "      <td>0.933087</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>12</td>\n",
       "      <td>0.932335</td>\n",
       "      <td>0.932058</td>\n",
       "      <td>0.939579</td>\n",
       "      <td>0.934657</td>\n",
       "      <td>0.003482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.028889</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 16}</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.929575</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>13</td>\n",
       "      <td>0.928175</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.938470</td>\n",
       "      <td>0.933178</td>\n",
       "      <td>0.004208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>15</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 512}</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.929575</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925957</td>\n",
       "      <td>0.932058</td>\n",
       "      <td>0.937361</td>\n",
       "      <td>0.931792</td>\n",
       "      <td>0.004660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.029386</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>15</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 256}</td>\n",
       "      <td>0.935144</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.929205</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>15</td>\n",
       "      <td>0.926789</td>\n",
       "      <td>0.932335</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.932254</td>\n",
       "      <td>0.004430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.030226</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 64}</td>\n",
       "      <td>0.934590</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.929020</td>\n",
       "      <td>0.009083</td>\n",
       "      <td>16</td>\n",
       "      <td>0.927898</td>\n",
       "      <td>0.932612</td>\n",
       "      <td>0.938193</td>\n",
       "      <td>0.932901</td>\n",
       "      <td>0.004208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.027721</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>15</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 128}</td>\n",
       "      <td>0.935144</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>0.928835</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>17</td>\n",
       "      <td>0.926789</td>\n",
       "      <td>0.932612</td>\n",
       "      <td>0.937916</td>\n",
       "      <td>0.932439</td>\n",
       "      <td>0.004544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.028778</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 32}</td>\n",
       "      <td>0.932373</td>\n",
       "      <td>0.936253</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.928281</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>18</td>\n",
       "      <td>0.927898</td>\n",
       "      <td>0.932612</td>\n",
       "      <td>0.938193</td>\n",
       "      <td>0.932901</td>\n",
       "      <td>0.004208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.026273</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>13</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 512}</td>\n",
       "      <td>0.929601</td>\n",
       "      <td>0.931264</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.925693</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>19</td>\n",
       "      <td>0.918192</td>\n",
       "      <td>0.924847</td>\n",
       "      <td>0.937361</td>\n",
       "      <td>0.926800</td>\n",
       "      <td>0.007947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 32}</td>\n",
       "      <td>0.929601</td>\n",
       "      <td>0.931264</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.925693</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>19</td>\n",
       "      <td>0.919856</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.937916</td>\n",
       "      <td>0.927725</td>\n",
       "      <td>0.007554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.026842</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>13</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 256}</td>\n",
       "      <td>0.929047</td>\n",
       "      <td>0.931264</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.925508</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>21</td>\n",
       "      <td>0.919024</td>\n",
       "      <td>0.925125</td>\n",
       "      <td>0.937361</td>\n",
       "      <td>0.927170</td>\n",
       "      <td>0.007625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.027680</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 64}</td>\n",
       "      <td>0.929047</td>\n",
       "      <td>0.931264</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.925508</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>21</td>\n",
       "      <td>0.919856</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.937916</td>\n",
       "      <td>0.927725</td>\n",
       "      <td>0.007554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.027547</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 16}</td>\n",
       "      <td>0.928492</td>\n",
       "      <td>0.931264</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.925323</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>23</td>\n",
       "      <td>0.919856</td>\n",
       "      <td>0.925679</td>\n",
       "      <td>0.938193</td>\n",
       "      <td>0.927909</td>\n",
       "      <td>0.007650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.027111</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>13</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 128}</td>\n",
       "      <td>0.928492</td>\n",
       "      <td>0.930710</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.925139</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>24</td>\n",
       "      <td>0.919024</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.927355</td>\n",
       "      <td>0.007724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.026050</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 32}</td>\n",
       "      <td>0.924612</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.919963</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>25</td>\n",
       "      <td>0.914032</td>\n",
       "      <td>0.915696</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.922456</td>\n",
       "      <td>0.010757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.025037</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>11</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 512}</td>\n",
       "      <td>0.924612</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.919963</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>25</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>0.915419</td>\n",
       "      <td>0.937084</td>\n",
       "      <td>0.921901</td>\n",
       "      <td>0.010774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>11</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 256}</td>\n",
       "      <td>0.924612</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.919963</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>25</td>\n",
       "      <td>0.913755</td>\n",
       "      <td>0.915419</td>\n",
       "      <td>0.937084</td>\n",
       "      <td>0.922086</td>\n",
       "      <td>0.010627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.026307</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>11</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 128}</td>\n",
       "      <td>0.924612</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.919963</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>25</td>\n",
       "      <td>0.913755</td>\n",
       "      <td>0.915696</td>\n",
       "      <td>0.937361</td>\n",
       "      <td>0.922271</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.025802</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 16}</td>\n",
       "      <td>0.924612</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.919963</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>25</td>\n",
       "      <td>0.914032</td>\n",
       "      <td>0.915973</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.922548</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.029499</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 64}</td>\n",
       "      <td>0.923503</td>\n",
       "      <td>0.918514</td>\n",
       "      <td>0.916204</td>\n",
       "      <td>0.919409</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>30</td>\n",
       "      <td>0.914032</td>\n",
       "      <td>0.915696</td>\n",
       "      <td>0.937639</td>\n",
       "      <td>0.922456</td>\n",
       "      <td>0.010757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.023891</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>9</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 512}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.909545</td>\n",
       "      <td>0.914233</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>31</td>\n",
       "      <td>0.899334</td>\n",
       "      <td>0.914864</td>\n",
       "      <td>0.928769</td>\n",
       "      <td>0.914323</td>\n",
       "      <td>0.012023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>9</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 256}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.909545</td>\n",
       "      <td>0.914233</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>31</td>\n",
       "      <td>0.899334</td>\n",
       "      <td>0.914864</td>\n",
       "      <td>0.928769</td>\n",
       "      <td>0.914323</td>\n",
       "      <td>0.012023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.024707</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>9</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 128}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.909545</td>\n",
       "      <td>0.914233</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>31</td>\n",
       "      <td>0.899334</td>\n",
       "      <td>0.915141</td>\n",
       "      <td>0.929047</td>\n",
       "      <td>0.914507</td>\n",
       "      <td>0.012138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.023718</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 32}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.909545</td>\n",
       "      <td>0.914233</td>\n",
       "      <td>0.003889</td>\n",
       "      <td>31</td>\n",
       "      <td>0.899612</td>\n",
       "      <td>0.915141</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.914692</td>\n",
       "      <td>0.012134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.024209</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 16}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.908435</td>\n",
       "      <td>0.913863</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>35</td>\n",
       "      <td>0.899612</td>\n",
       "      <td>0.915419</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.914785</td>\n",
       "      <td>0.012138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.023661</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>9</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 64}</td>\n",
       "      <td>0.911863</td>\n",
       "      <td>0.919069</td>\n",
       "      <td>0.909545</td>\n",
       "      <td>0.913494</td>\n",
       "      <td>0.004055</td>\n",
       "      <td>36</td>\n",
       "      <td>0.899612</td>\n",
       "      <td>0.915141</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.914692</td>\n",
       "      <td>0.012134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.021045</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>7</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 512}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.895117</td>\n",
       "      <td>0.907948</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>37</td>\n",
       "      <td>0.899334</td>\n",
       "      <td>0.909040</td>\n",
       "      <td>0.917960</td>\n",
       "      <td>0.908778</td>\n",
       "      <td>0.007606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.021513</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 256}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.895117</td>\n",
       "      <td>0.907948</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>37</td>\n",
       "      <td>0.899334</td>\n",
       "      <td>0.909040</td>\n",
       "      <td>0.917960</td>\n",
       "      <td>0.908778</td>\n",
       "      <td>0.007606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.021315</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>7</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 128}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.895117</td>\n",
       "      <td>0.907948</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>37</td>\n",
       "      <td>0.899334</td>\n",
       "      <td>0.909318</td>\n",
       "      <td>0.918237</td>\n",
       "      <td>0.908963</td>\n",
       "      <td>0.007721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 64}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.895117</td>\n",
       "      <td>0.907948</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>37</td>\n",
       "      <td>0.899612</td>\n",
       "      <td>0.909318</td>\n",
       "      <td>0.918514</td>\n",
       "      <td>0.909148</td>\n",
       "      <td>0.007718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.021879</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 32}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.895117</td>\n",
       "      <td>0.907948</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>37</td>\n",
       "      <td>0.899612</td>\n",
       "      <td>0.909318</td>\n",
       "      <td>0.918514</td>\n",
       "      <td>0.909148</td>\n",
       "      <td>0.007718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.021524</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 16}</td>\n",
       "      <td>0.914080</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.895117</td>\n",
       "      <td>0.907948</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>37</td>\n",
       "      <td>0.899612</td>\n",
       "      <td>0.909318</td>\n",
       "      <td>0.918514</td>\n",
       "      <td>0.909148</td>\n",
       "      <td>0.007718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.020627</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 64}</td>\n",
       "      <td>0.905765</td>\n",
       "      <td>0.895787</td>\n",
       "      <td>0.880133</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>43</td>\n",
       "      <td>0.887965</td>\n",
       "      <td>0.893233</td>\n",
       "      <td>0.901053</td>\n",
       "      <td>0.894084</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.021209</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 16}</td>\n",
       "      <td>0.905765</td>\n",
       "      <td>0.895787</td>\n",
       "      <td>0.880133</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>43</td>\n",
       "      <td>0.887965</td>\n",
       "      <td>0.893233</td>\n",
       "      <td>0.901053</td>\n",
       "      <td>0.894084</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.020819</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 32}</td>\n",
       "      <td>0.905765</td>\n",
       "      <td>0.895787</td>\n",
       "      <td>0.880133</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>43</td>\n",
       "      <td>0.887965</td>\n",
       "      <td>0.893233</td>\n",
       "      <td>0.901053</td>\n",
       "      <td>0.894084</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.020195</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 256}</td>\n",
       "      <td>0.905765</td>\n",
       "      <td>0.895787</td>\n",
       "      <td>0.880133</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>43</td>\n",
       "      <td>0.887965</td>\n",
       "      <td>0.892956</td>\n",
       "      <td>0.900776</td>\n",
       "      <td>0.893899</td>\n",
       "      <td>0.005273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.020594</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 128}</td>\n",
       "      <td>0.905765</td>\n",
       "      <td>0.895787</td>\n",
       "      <td>0.880133</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>43</td>\n",
       "      <td>0.887965</td>\n",
       "      <td>0.893233</td>\n",
       "      <td>0.901053</td>\n",
       "      <td>0.894084</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>5</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 512}</td>\n",
       "      <td>0.905765</td>\n",
       "      <td>0.895787</td>\n",
       "      <td>0.880133</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.010548</td>\n",
       "      <td>43</td>\n",
       "      <td>0.887965</td>\n",
       "      <td>0.892956</td>\n",
       "      <td>0.900776</td>\n",
       "      <td>0.893899</td>\n",
       "      <td>0.005273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.020153</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 512}</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>0.839246</td>\n",
       "      <td>0.830744</td>\n",
       "      <td>0.844362</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>49</td>\n",
       "      <td>0.834997</td>\n",
       "      <td>0.846922</td>\n",
       "      <td>0.851164</td>\n",
       "      <td>0.844361</td>\n",
       "      <td>0.006844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.019196</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 256}</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>0.839246</td>\n",
       "      <td>0.830744</td>\n",
       "      <td>0.844362</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>49</td>\n",
       "      <td>0.834997</td>\n",
       "      <td>0.846922</td>\n",
       "      <td>0.851164</td>\n",
       "      <td>0.844361</td>\n",
       "      <td>0.006844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.019490</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 128}</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>0.839246</td>\n",
       "      <td>0.830744</td>\n",
       "      <td>0.844362</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>49</td>\n",
       "      <td>0.834997</td>\n",
       "      <td>0.846922</td>\n",
       "      <td>0.851164</td>\n",
       "      <td>0.844361</td>\n",
       "      <td>0.006844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.019189</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 64}</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>0.839246</td>\n",
       "      <td>0.830744</td>\n",
       "      <td>0.844362</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>49</td>\n",
       "      <td>0.834997</td>\n",
       "      <td>0.846922</td>\n",
       "      <td>0.851164</td>\n",
       "      <td>0.844361</td>\n",
       "      <td>0.006844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.017881</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 32}</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>0.839246</td>\n",
       "      <td>0.830744</td>\n",
       "      <td>0.844362</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>49</td>\n",
       "      <td>0.834997</td>\n",
       "      <td>0.846922</td>\n",
       "      <td>0.851164</td>\n",
       "      <td>0.844361</td>\n",
       "      <td>0.006844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.018322</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 16}</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>0.839246</td>\n",
       "      <td>0.830744</td>\n",
       "      <td>0.844362</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>49</td>\n",
       "      <td>0.834997</td>\n",
       "      <td>0.846922</td>\n",
       "      <td>0.851164</td>\n",
       "      <td>0.844361</td>\n",
       "      <td>0.006844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016159</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 32}</td>\n",
       "      <td>0.754989</td>\n",
       "      <td>0.741131</td>\n",
       "      <td>0.725860</td>\n",
       "      <td>0.740665</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>55</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.740433</td>\n",
       "      <td>0.748060</td>\n",
       "      <td>0.740664</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016003</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 512}</td>\n",
       "      <td>0.754989</td>\n",
       "      <td>0.741131</td>\n",
       "      <td>0.725860</td>\n",
       "      <td>0.740665</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>55</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.740433</td>\n",
       "      <td>0.748060</td>\n",
       "      <td>0.740664</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 256}</td>\n",
       "      <td>0.754989</td>\n",
       "      <td>0.741131</td>\n",
       "      <td>0.725860</td>\n",
       "      <td>0.740665</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>55</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.740433</td>\n",
       "      <td>0.748060</td>\n",
       "      <td>0.740664</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016630</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 128}</td>\n",
       "      <td>0.754989</td>\n",
       "      <td>0.741131</td>\n",
       "      <td>0.725860</td>\n",
       "      <td>0.740665</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>55</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.740433</td>\n",
       "      <td>0.748060</td>\n",
       "      <td>0.740664</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 64}</td>\n",
       "      <td>0.754989</td>\n",
       "      <td>0.741131</td>\n",
       "      <td>0.725860</td>\n",
       "      <td>0.740665</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>55</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.740433</td>\n",
       "      <td>0.748060</td>\n",
       "      <td>0.740664</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017405</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 16}</td>\n",
       "      <td>0.754989</td>\n",
       "      <td>0.741131</td>\n",
       "      <td>0.725860</td>\n",
       "      <td>0.740665</td>\n",
       "      <td>0.011895</td>\n",
       "      <td>55</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.740433</td>\n",
       "      <td>0.748060</td>\n",
       "      <td>0.740664</td>\n",
       "      <td>0.005946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "58       0.030818      0.001451         0.001100        0.000111   \n",
       "56       0.030393      0.001168         0.001119        0.000159   \n",
       "57       0.030607      0.001319         0.001002        0.000003   \n",
       "54       0.032033      0.000858         0.001151        0.000177   \n",
       "55       0.030172      0.001510         0.001047        0.000185   \n",
       "52       0.028520      0.001433         0.000977        0.000080   \n",
       "59       0.029229      0.001384         0.001033        0.000052   \n",
       "51       0.028502      0.001495         0.000905        0.000032   \n",
       "50       0.028950      0.001101         0.000959        0.000065   \n",
       "49       0.031900      0.002356         0.001271        0.000206   \n",
       "48       0.031870      0.001624         0.001142        0.000110   \n",
       "53       0.029390      0.002118         0.001180        0.000077   \n",
       "42       0.028889      0.001188         0.001232        0.000149   \n",
       "47       0.027514      0.001601         0.001010        0.000006   \n",
       "46       0.029386      0.000707         0.001134        0.000118   \n",
       "44       0.030226      0.001150         0.001033        0.000009   \n",
       "45       0.027721      0.001178         0.000970        0.000056   \n",
       "43       0.028778      0.001200         0.001017        0.000008   \n",
       "41       0.026273      0.001429         0.000994        0.000030   \n",
       "37       0.028229      0.001614         0.001011        0.000068   \n",
       "40       0.026842      0.000867         0.001092        0.000091   \n",
       "38       0.027680      0.000790         0.000971        0.000007   \n",
       "36       0.027547      0.000887         0.001119        0.000125   \n",
       "39       0.027111      0.001115         0.001028        0.000082   \n",
       "31       0.026050      0.001309         0.001217        0.000168   \n",
       "35       0.025037      0.001484         0.000969        0.000016   \n",
       "34       0.025001      0.000589         0.001106        0.000169   \n",
       "33       0.026307      0.000920         0.001045        0.000097   \n",
       "30       0.025802      0.000226         0.001031        0.000082   \n",
       "32       0.029499      0.001685         0.001300        0.000184   \n",
       "29       0.023891      0.000652         0.001071        0.000164   \n",
       "28       0.023733      0.000683         0.001128        0.000134   \n",
       "27       0.024707      0.001472         0.001094        0.000122   \n",
       "25       0.023718      0.000985         0.000994        0.000014   \n",
       "24       0.024209      0.001259         0.001004        0.000043   \n",
       "26       0.023661      0.001518         0.001007        0.000084   \n",
       "23       0.021045      0.000892         0.000912        0.000038   \n",
       "22       0.021513      0.000335         0.000962        0.000025   \n",
       "21       0.021315      0.000701         0.000941        0.000036   \n",
       "20       0.021558      0.000813         0.000934        0.000032   \n",
       "19       0.021879      0.000791         0.000990        0.000058   \n",
       "18       0.021524      0.000863         0.000909        0.000035   \n",
       "14       0.020627      0.000848         0.000913        0.000018   \n",
       "12       0.021209      0.000483         0.000957        0.000085   \n",
       "13       0.020819      0.000839         0.001186        0.000290   \n",
       "16       0.020195      0.000643         0.000943        0.000049   \n",
       "15       0.020594      0.000623         0.001023        0.000025   \n",
       "17       0.020439      0.001048         0.001030        0.000097   \n",
       "11       0.020153      0.001466         0.001112        0.000127   \n",
       "10       0.019196      0.000704         0.001239        0.000371   \n",
       "9        0.019490      0.001171         0.001160        0.000198   \n",
       "8        0.019189      0.001936         0.001149        0.000164   \n",
       "7        0.017881      0.000447         0.000921        0.000057   \n",
       "6        0.018322      0.000307         0.000929        0.000037   \n",
       "1        0.016159      0.000577         0.000926        0.000053   \n",
       "5        0.016003      0.000531         0.000790        0.000034   \n",
       "4        0.016709      0.000547         0.000981        0.000130   \n",
       "3        0.016630      0.001245         0.000857        0.000061   \n",
       "2        0.016109      0.000570         0.000796        0.000019   \n",
       "0        0.017405      0.000196         0.001467        0.000733   \n",
       "\n",
       "   param_max_depth param_min_samples_split  \\\n",
       "58              19                     256   \n",
       "56              19                      64   \n",
       "57              19                     128   \n",
       "54              19                      16   \n",
       "55              19                      32   \n",
       "52              17                     256   \n",
       "59              19                     512   \n",
       "51              17                     128   \n",
       "50              17                      64   \n",
       "49              17                      32   \n",
       "48              17                      16   \n",
       "53              17                     512   \n",
       "42              15                      16   \n",
       "47              15                     512   \n",
       "46              15                     256   \n",
       "44              15                      64   \n",
       "45              15                     128   \n",
       "43              15                      32   \n",
       "41              13                     512   \n",
       "37              13                      32   \n",
       "40              13                     256   \n",
       "38              13                      64   \n",
       "36              13                      16   \n",
       "39              13                     128   \n",
       "31              11                      32   \n",
       "35              11                     512   \n",
       "34              11                     256   \n",
       "33              11                     128   \n",
       "30              11                      16   \n",
       "32              11                      64   \n",
       "29               9                     512   \n",
       "28               9                     256   \n",
       "27               9                     128   \n",
       "25               9                      32   \n",
       "24               9                      16   \n",
       "26               9                      64   \n",
       "23               7                     512   \n",
       "22               7                     256   \n",
       "21               7                     128   \n",
       "20               7                      64   \n",
       "19               7                      32   \n",
       "18               7                      16   \n",
       "14               5                      64   \n",
       "12               5                      16   \n",
       "13               5                      32   \n",
       "16               5                     256   \n",
       "15               5                     128   \n",
       "17               5                     512   \n",
       "11               3                     512   \n",
       "10               3                     256   \n",
       "9                3                     128   \n",
       "8                3                      64   \n",
       "7                3                      32   \n",
       "6                3                      16   \n",
       "1                1                      32   \n",
       "5                1                     512   \n",
       "4                1                     256   \n",
       "3                1                     128   \n",
       "2                1                      64   \n",
       "0                1                      16   \n",
       "\n",
       "                                         params  split0_test_score  \\\n",
       "58  {'max_depth': 19, 'min_samples_split': 256}           0.940687   \n",
       "56   {'max_depth': 19, 'min_samples_split': 64}           0.941796   \n",
       "57  {'max_depth': 19, 'min_samples_split': 128}           0.940687   \n",
       "54   {'max_depth': 19, 'min_samples_split': 16}           0.941242   \n",
       "55   {'max_depth': 19, 'min_samples_split': 32}           0.940687   \n",
       "52  {'max_depth': 17, 'min_samples_split': 256}           0.941796   \n",
       "59  {'max_depth': 19, 'min_samples_split': 512}           0.941796   \n",
       "51  {'max_depth': 17, 'min_samples_split': 128}           0.941242   \n",
       "50   {'max_depth': 17, 'min_samples_split': 64}           0.940687   \n",
       "49   {'max_depth': 17, 'min_samples_split': 32}           0.939579   \n",
       "48   {'max_depth': 17, 'min_samples_split': 16}           0.939579   \n",
       "53  {'max_depth': 17, 'min_samples_split': 512}           0.941796   \n",
       "42   {'max_depth': 15, 'min_samples_split': 16}           0.936253   \n",
       "47  {'max_depth': 15, 'min_samples_split': 512}           0.936253   \n",
       "46  {'max_depth': 15, 'min_samples_split': 256}           0.935144   \n",
       "44   {'max_depth': 15, 'min_samples_split': 64}           0.934590   \n",
       "45  {'max_depth': 15, 'min_samples_split': 128}           0.935144   \n",
       "43   {'max_depth': 15, 'min_samples_split': 32}           0.932373   \n",
       "41  {'max_depth': 13, 'min_samples_split': 512}           0.929601   \n",
       "37   {'max_depth': 13, 'min_samples_split': 32}           0.929601   \n",
       "40  {'max_depth': 13, 'min_samples_split': 256}           0.929047   \n",
       "38   {'max_depth': 13, 'min_samples_split': 64}           0.929047   \n",
       "36   {'max_depth': 13, 'min_samples_split': 16}           0.928492   \n",
       "39  {'max_depth': 13, 'min_samples_split': 128}           0.928492   \n",
       "31   {'max_depth': 11, 'min_samples_split': 32}           0.924612   \n",
       "35  {'max_depth': 11, 'min_samples_split': 512}           0.924612   \n",
       "34  {'max_depth': 11, 'min_samples_split': 256}           0.924612   \n",
       "33  {'max_depth': 11, 'min_samples_split': 128}           0.924612   \n",
       "30   {'max_depth': 11, 'min_samples_split': 16}           0.924612   \n",
       "32   {'max_depth': 11, 'min_samples_split': 64}           0.923503   \n",
       "29   {'max_depth': 9, 'min_samples_split': 512}           0.914080   \n",
       "28   {'max_depth': 9, 'min_samples_split': 256}           0.914080   \n",
       "27   {'max_depth': 9, 'min_samples_split': 128}           0.914080   \n",
       "25    {'max_depth': 9, 'min_samples_split': 32}           0.914080   \n",
       "24    {'max_depth': 9, 'min_samples_split': 16}           0.914080   \n",
       "26    {'max_depth': 9, 'min_samples_split': 64}           0.911863   \n",
       "23   {'max_depth': 7, 'min_samples_split': 512}           0.914080   \n",
       "22   {'max_depth': 7, 'min_samples_split': 256}           0.914080   \n",
       "21   {'max_depth': 7, 'min_samples_split': 128}           0.914080   \n",
       "20    {'max_depth': 7, 'min_samples_split': 64}           0.914080   \n",
       "19    {'max_depth': 7, 'min_samples_split': 32}           0.914080   \n",
       "18    {'max_depth': 7, 'min_samples_split': 16}           0.914080   \n",
       "14    {'max_depth': 5, 'min_samples_split': 64}           0.905765   \n",
       "12    {'max_depth': 5, 'min_samples_split': 16}           0.905765   \n",
       "13    {'max_depth': 5, 'min_samples_split': 32}           0.905765   \n",
       "16   {'max_depth': 5, 'min_samples_split': 256}           0.905765   \n",
       "15   {'max_depth': 5, 'min_samples_split': 128}           0.905765   \n",
       "17   {'max_depth': 5, 'min_samples_split': 512}           0.905765   \n",
       "11   {'max_depth': 3, 'min_samples_split': 512}           0.863082   \n",
       "10   {'max_depth': 3, 'min_samples_split': 256}           0.863082   \n",
       "9    {'max_depth': 3, 'min_samples_split': 128}           0.863082   \n",
       "8     {'max_depth': 3, 'min_samples_split': 64}           0.863082   \n",
       "7     {'max_depth': 3, 'min_samples_split': 32}           0.863082   \n",
       "6     {'max_depth': 3, 'min_samples_split': 16}           0.863082   \n",
       "1     {'max_depth': 1, 'min_samples_split': 32}           0.754989   \n",
       "5    {'max_depth': 1, 'min_samples_split': 512}           0.754989   \n",
       "4    {'max_depth': 1, 'min_samples_split': 256}           0.754989   \n",
       "3    {'max_depth': 1, 'min_samples_split': 128}           0.754989   \n",
       "2     {'max_depth': 1, 'min_samples_split': 64}           0.754989   \n",
       "0     {'max_depth': 1, 'min_samples_split': 16}           0.754989   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "58           0.939579           0.930078         0.936784        0.004761   \n",
       "56           0.937916           0.930078         0.936599        0.004873   \n",
       "57           0.937916           0.929523         0.936044        0.004746   \n",
       "54           0.937916           0.928968         0.936044        0.005182   \n",
       "55           0.937916           0.928968         0.935860        0.005000   \n",
       "52           0.936253           0.925083         0.934381        0.006950   \n",
       "59           0.937916           0.922863         0.934196        0.008164   \n",
       "51           0.936253           0.925083         0.934196        0.006755   \n",
       "50           0.935144           0.925083         0.933641        0.006458   \n",
       "49           0.936253           0.925083         0.933641        0.006199   \n",
       "48           0.936253           0.925083         0.933641        0.006199   \n",
       "53           0.936253           0.921199         0.933087        0.008701   \n",
       "42           0.936253           0.916204         0.929575        0.009449   \n",
       "47           0.936253           0.916204         0.929575        0.009449   \n",
       "46           0.936253           0.916204         0.929205        0.009199   \n",
       "44           0.936253           0.916204         0.929020        0.009083   \n",
       "45           0.936253           0.915094         0.928835        0.009722   \n",
       "43           0.936253           0.916204         0.928281        0.008681   \n",
       "41           0.931264           0.916204         0.925693        0.006740   \n",
       "37           0.931264           0.916204         0.925693        0.006740   \n",
       "40           0.931264           0.916204         0.925508        0.006637   \n",
       "38           0.931264           0.916204         0.925508        0.006637   \n",
       "36           0.931264           0.916204         0.925323        0.006543   \n",
       "39           0.930710           0.916204         0.925139        0.006379   \n",
       "31           0.919069           0.916204         0.919963        0.003490   \n",
       "35           0.919069           0.916204         0.919963        0.003490   \n",
       "34           0.919069           0.916204         0.919963        0.003490   \n",
       "33           0.919069           0.916204         0.919963        0.003490   \n",
       "30           0.919069           0.916204         0.919963        0.003490   \n",
       "32           0.918514           0.916204         0.919409        0.003046   \n",
       "29           0.919069           0.909545         0.914233        0.003889   \n",
       "28           0.919069           0.909545         0.914233        0.003889   \n",
       "27           0.919069           0.909545         0.914233        0.003889   \n",
       "25           0.919069           0.909545         0.914233        0.003889   \n",
       "24           0.919069           0.908435         0.913863        0.004343   \n",
       "26           0.919069           0.909545         0.913494        0.004055   \n",
       "23           0.914634           0.895117         0.907948        0.009071   \n",
       "22           0.914634           0.895117         0.907948        0.009071   \n",
       "21           0.914634           0.895117         0.907948        0.009071   \n",
       "20           0.914634           0.895117         0.907948        0.009071   \n",
       "19           0.914634           0.895117         0.907948        0.009071   \n",
       "18           0.914634           0.895117         0.907948        0.009071   \n",
       "14           0.895787           0.880133         0.893900        0.010548   \n",
       "12           0.895787           0.880133         0.893900        0.010548   \n",
       "13           0.895787           0.880133         0.893900        0.010548   \n",
       "16           0.895787           0.880133         0.893900        0.010548   \n",
       "15           0.895787           0.880133         0.893900        0.010548   \n",
       "17           0.895787           0.880133         0.893900        0.010548   \n",
       "11           0.839246           0.830744         0.844362        0.013688   \n",
       "10           0.839246           0.830744         0.844362        0.013688   \n",
       "9            0.839246           0.830744         0.844362        0.013688   \n",
       "8            0.839246           0.830744         0.844362        0.013688   \n",
       "7            0.839246           0.830744         0.844362        0.013688   \n",
       "6            0.839246           0.830744         0.844362        0.013688   \n",
       "1            0.741131           0.725860         0.740665        0.011895   \n",
       "5            0.741131           0.725860         0.740665        0.011895   \n",
       "4            0.741131           0.725860         0.740665        0.011895   \n",
       "3            0.741131           0.725860         0.740665        0.011895   \n",
       "2            0.741131           0.725860         0.740665        0.011895   \n",
       "0            0.741131           0.725860         0.740665        0.011895   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "58                1            0.933167            0.934554   \n",
       "56                2            0.934276            0.935108   \n",
       "57                3            0.933167            0.934831   \n",
       "54                3            0.934554            0.935385   \n",
       "55                5            0.934276            0.935108   \n",
       "52                6            0.933167            0.932335   \n",
       "59                7            0.932335            0.934276   \n",
       "51                7            0.933167            0.932612   \n",
       "50                9            0.934276            0.932890   \n",
       "49                9            0.934276            0.932890   \n",
       "48                9            0.934554            0.933167   \n",
       "53               12            0.932335            0.932058   \n",
       "42               13            0.928175            0.932890   \n",
       "47               13            0.925957            0.932058   \n",
       "46               15            0.926789            0.932335   \n",
       "44               16            0.927898            0.932612   \n",
       "45               17            0.926789            0.932612   \n",
       "43               18            0.927898            0.932612   \n",
       "41               19            0.918192            0.924847   \n",
       "37               19            0.919856            0.925402   \n",
       "40               21            0.919024            0.925125   \n",
       "38               21            0.919856            0.925402   \n",
       "36               23            0.919856            0.925679   \n",
       "39               24            0.919024            0.925402   \n",
       "31               25            0.914032            0.915696   \n",
       "35               25            0.913200            0.915419   \n",
       "34               25            0.913755            0.915419   \n",
       "33               25            0.913755            0.915696   \n",
       "30               25            0.914032            0.915973   \n",
       "32               30            0.914032            0.915696   \n",
       "29               31            0.899334            0.914864   \n",
       "28               31            0.899334            0.914864   \n",
       "27               31            0.899334            0.915141   \n",
       "25               31            0.899612            0.915141   \n",
       "24               35            0.899612            0.915419   \n",
       "26               36            0.899612            0.915141   \n",
       "23               37            0.899334            0.909040   \n",
       "22               37            0.899334            0.909040   \n",
       "21               37            0.899334            0.909318   \n",
       "20               37            0.899612            0.909318   \n",
       "19               37            0.899612            0.909318   \n",
       "18               37            0.899612            0.909318   \n",
       "14               43            0.887965            0.893233   \n",
       "12               43            0.887965            0.893233   \n",
       "13               43            0.887965            0.893233   \n",
       "16               43            0.887965            0.892956   \n",
       "15               43            0.887965            0.893233   \n",
       "17               43            0.887965            0.892956   \n",
       "11               49            0.834997            0.846922   \n",
       "10               49            0.834997            0.846922   \n",
       "9                49            0.834997            0.846922   \n",
       "8                49            0.834997            0.846922   \n",
       "7                49            0.834997            0.846922   \n",
       "6                49            0.834997            0.846922   \n",
       "1                55            0.733500            0.740433   \n",
       "5                55            0.733500            0.740433   \n",
       "4                55            0.733500            0.740433   \n",
       "3                55            0.733500            0.740433   \n",
       "2                55            0.733500            0.740433   \n",
       "0                55            0.733500            0.740433   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "58            0.948448          0.938723         0.006900  \n",
       "56            0.949002          0.939462         0.006754  \n",
       "57            0.948725          0.938908         0.006975  \n",
       "54            0.949279          0.939739         0.006754  \n",
       "55            0.949002          0.939462         0.006754  \n",
       "52            0.944013          0.936505         0.005320  \n",
       "59            0.941242          0.935951         0.003824  \n",
       "51            0.944290          0.936690         0.005379  \n",
       "50            0.944568          0.937244         0.005209  \n",
       "49            0.944568          0.937244         0.005209  \n",
       "48            0.944845          0.937522         0.005209  \n",
       "53            0.939579          0.934657         0.003482  \n",
       "42            0.938470          0.933178         0.004208  \n",
       "47            0.937361          0.931792         0.004660  \n",
       "46            0.937639          0.932254         0.004430  \n",
       "44            0.938193          0.932901         0.004208  \n",
       "45            0.937916          0.932439         0.004544  \n",
       "43            0.938193          0.932901         0.004208  \n",
       "41            0.937361          0.926800         0.007947  \n",
       "37            0.937916          0.927725         0.007554  \n",
       "40            0.937361          0.927170         0.007625  \n",
       "38            0.937916          0.927725         0.007554  \n",
       "36            0.938193          0.927909         0.007650  \n",
       "39            0.937639          0.927355         0.007724  \n",
       "31            0.937639          0.922456         0.010757  \n",
       "35            0.937084          0.921901         0.010774  \n",
       "34            0.937084          0.922086         0.010627  \n",
       "33            0.937361          0.922271         0.010700  \n",
       "30            0.937639          0.922548         0.010700  \n",
       "32            0.937639          0.922456         0.010757  \n",
       "29            0.928769          0.914323         0.012023  \n",
       "28            0.928769          0.914323         0.012023  \n",
       "27            0.929047          0.914507         0.012138  \n",
       "25            0.929324          0.914692         0.012134  \n",
       "24            0.929324          0.914785         0.012138  \n",
       "26            0.929324          0.914692         0.012134  \n",
       "23            0.917960          0.908778         0.007606  \n",
       "22            0.917960          0.908778         0.007606  \n",
       "21            0.918237          0.908963         0.007721  \n",
       "20            0.918514          0.909148         0.007718  \n",
       "19            0.918514          0.909148         0.007718  \n",
       "18            0.918514          0.909148         0.007718  \n",
       "14            0.901053          0.894084         0.005377  \n",
       "12            0.901053          0.894084         0.005377  \n",
       "13            0.901053          0.894084         0.005377  \n",
       "16            0.900776          0.893899         0.005273  \n",
       "15            0.901053          0.894084         0.005377  \n",
       "17            0.900776          0.893899         0.005273  \n",
       "11            0.851164          0.844361         0.006844  \n",
       "10            0.851164          0.844361         0.006844  \n",
       "9             0.851164          0.844361         0.006844  \n",
       "8             0.851164          0.844361         0.006844  \n",
       "7             0.851164          0.844361         0.006844  \n",
       "6             0.851164          0.844361         0.006844  \n",
       "1             0.748060          0.740664         0.005946  \n",
       "5             0.748060          0.740664         0.005946  \n",
       "4             0.748060          0.740664         0.005946  \n",
       "3             0.748060          0.740664         0.005946  \n",
       "2             0.748060          0.740664         0.005946  \n",
       "0             0.748060          0.740664         0.005946  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found {'max_depth': 19, 'min_samples_split': 256}\n"
     ]
    }
   ],
   "source": [
    "import os, urllib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def load_branche_data(keys):\n",
    "    \"\"\"\n",
    "    Load the data in branche_data.npz and save it in lists of\n",
    "    strings and labels (whose entries are in {0,1,..,num_classes-1})\n",
    "    \"\"\"\n",
    "    filename = 'branchekoder_formal.gzip'\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'wb') as fh:\n",
    "            path = \"http://users-cs.au.dk/jallan/ml/data/{0}\".format(filename)\n",
    "            fh.write(urllib.request.urlopen(path).read())\n",
    "    data = pd.read_csv(filename, compression='gzip')\n",
    "    actual_class_names = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i, kv in enumerate(keys):\n",
    "        key = kv[0]\n",
    "        name = kv[1]\n",
    "        strings = data[data.branchekode == key].formal.values\n",
    "        features.extend(list(strings))\n",
    "        label = [i] * len(strings)\n",
    "        labels.extend(label)\n",
    "        actual_class_names.append(name)\n",
    "    assert len(features) == len(labels)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels, actual_class_names\n",
    "\n",
    "\n",
    "def get_branche_data(keys):\n",
    "    features, labels, actual_class_names = load_branche_data(keys)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "    return X_train, X_test, y_train, y_test, actual_class_names\n",
    "\n",
    "\n",
    "keys = [(561010, 'Restauranter'), (620100, 'Computerprogrammering')]\n",
    "feat_train, feat_test, lab_train, lab_test, cnames = get_branche_data(keys)\n",
    "\n",
    "def decisiontree_model_selection(feat_train, feat_test):\n",
    "    c = CountVectorizer()\n",
    "    c.fit(feat_train)\n",
    "    bag_of_words_feat_train = c.transform(feat_train)\n",
    "    bag_of_words_feat_test = c.transform(feat_test)\n",
    "    clf = None\n",
    "    ### YOUR CODE HERE\n",
    "    reg_parameters = {\n",
    "        'max_depth': list(range(1,20,2)),\n",
    "        'min_samples_split': [2**(i+4) for i in range(6)]\n",
    "    }\n",
    "    clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "    clf.fit(bag_of_words_feat_train, lab_train)\n",
    "    ### END CODE\n",
    "    return clf\n",
    "###\n",
    "clf = decisiontree_model_selection(feat_train, feat_test)\n",
    "bt = show_result(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 8: The Wrong and Right Way to Do (Cross) Validation\n",
    "In this exercise we will try and see a way of using cross validation for learning with $d$ features $N$ data points with $d>>N$.\n",
    "The idea would to do something as follows.\n",
    "\n",
    "- Step 1: Screen predictors: \n",
    "    Select predictors that correlate best with target (find best features)\n",
    "\n",
    "- Step 2. Simplify input: \n",
    "\tBuild classifier using  good predictors only \n",
    "\n",
    "- Step 3. Use cross-validation:\n",
    "\t Estimate tuning parameters and Eout (there will be no parameter tuning in our experiment - just measure eout)\n",
    "     \n",
    "### The Experiment - Cross Validation for Binary Classification\n",
    "<!--Given $D=\\{(x_1, y_1),...,(x_{50}, y_{50})\\}$ where $x_i\\in \\mathbb{R}^{5000}$ and $y_i\\in \\{0, 1\\}$. Let us assume 25 $y_i$'s are $0$'s and the remainding $25$ $y_i$'s are $1$'s. Furthermore, each entry of $x_i$ are sampled from a standard Gaussian independent of the target\n",
    "\n",
    "It might be difficult to handle $D$ because $n=50<<d=5000$, the number of points is much smaller than each points dimension! To handle this, -->\n",
    "\n",
    "\n",
    "**Setup:**\n",
    "- Input: 5000 features, 50 data points. In other words let $D=\\{(x_1, y_1),...,(x_{50}, y_{50})\\}$ where $x_i\\in \\mathbb{R}^{5000}$ and $y_i\\in \\{0, 1\\}$. \n",
    "- Let 25 points have $y_i=1$ and the remainding 25 points have $y_i=0$.  \n",
    "- Each feature is sampled from standard Gaussian independent of target. \n",
    "\n",
    "Notice that the number of points is much smaller than each points dimension, that is, $n=50<<d=5000$. \n",
    "\n",
    "**Learning Algorithm:**\n",
    "- Find the 100 features [correlating](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html) highest with target data (they are probably the most usefull and  5000 features for 50 data points seems to many) \n",
    "\n",
    "- Use the <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#The_1-nearest_neighbour_classifier\">1-Nearest Neighbor Classifier</a>:<br> \n",
    "We have not covered that but is is really easy to understand what it does. \n",
    "Given test data $x$ the algorithm finds the nearest point (under euclidian norm or some other predefined choice) in the dataset and returns that label. So learning algorithm just stores the data set for later use somehow!.\n",
    "- Estimate out of sample error with cross validation. Use 40 points to do Nearest Neighbor Classification and evaluate the performance on the remainding 10 points.   \n",
    "\n",
    "This gives CV error around 0-6%. Is this correct? Why, why not.\n",
    "\n",
    "You can see the experiment implemented in python below. \n",
    "\n",
    "**Task:** If you think there is something wrong you can try and make a new better experiment that shows the right way to do Cross Validation and then show experimentally what the error should be in this scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# If you get \"ImportError\" on this line you probably have an old version of sklearn < 0.18\n",
    "# (On linux you can update by 'pip uninstall scikit-learn' then 'pip install scikit-learn')\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "def run_exp():\n",
    "    nr_feat = 5000\n",
    "    nr_points = 50\n",
    "    dat = np.random.randn(nr_points, nr_feat)\n",
    "    \n",
    "    cs = int(nr_points/2)\n",
    "    target = np.ones(nr_points)\n",
    "    target[:cs] = 0\n",
    "    np.random.shuffle(target)\n",
    "    #print('target sum',target.sum())\n",
    "    cor = np.array([np.corrcoef(dat[:, i], target)[0,1] for i in range(nr_feat)])\n",
    "\n",
    "    idx = np.argsort(cor)[::-1]\n",
    "    #print('cor of first features',cor[idx[0:100]])\n",
    "    feat_to_use = dat[:,idx[0:100]]\n",
    "    X = feat_to_use\n",
    "    y = target\n",
    "    print('mean correlations in top 100 data', np.mean(cor[idx[0:100]]))\n",
    "    print('mean correlations in all data', np.mean(cor))\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    cv_errors = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        cor10 = np.array([np.corrcoef(X_test[:, i], y_test)[0,1] for i in range(100)])\n",
    "        print('cor between X_test and y_test ', cor10.mean())\n",
    "        nn_classifier = KNN(n_neighbors=1, algorithm='brute').fit(X_train, y_train)\n",
    "        nn_predictions = nn_classifier.predict(X_test)\n",
    "        error_prob = (nn_predictions != y_test).mean()\n",
    "        print('\\nSplit {0}:'.format(i))\n",
    "        print('Nearest Neighbour Cross Validation Error:', error_prob)\n",
    "        acc = 1-error_prob\n",
    "        print('Nearest Neighbour Cross Validation Accuracy:', acc)        \n",
    "        cv_errors.append(error_prob)\n",
    "    \n",
    "    print('*'*10)\n",
    "    print('Average Cross Validation Error {0}'.format(np.mean(np.array(cv_errors))))\n",
    "\n",
    "run_exp()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
